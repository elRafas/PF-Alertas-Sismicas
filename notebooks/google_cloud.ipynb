{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Creamos los permisos con la llave para ingresar a Google \n",
    "# activamos el servicio\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../notebooks/Clave_Google/henry-sismo-b50b99db8b10.json\"\n",
    "\n",
    "project_id = 'your-project-id'\n",
    "client = bigquery.Client(project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-06-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '-5.266',\n",
    "    'maxlatitude': '15.708',\n",
    "    'minlongitude': '276.328',\n",
    "    'maxlongitude': '293.906',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismo'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Colombia2'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField('id', 'STRING'),\n",
    "    bigquery.SchemaField('magnitude', 'FLOAT'),\n",
    "    bigquery.SchemaField('place', 'STRING'),\n",
    "    bigquery.SchemaField('time', 'TIMESTAMP'),\n",
    "    bigquery.SchemaField('url', 'STRING')\n",
    "    # Agrega más campos según los datos que desees almacenar\n",
    "]\n",
    "\n",
    "table = bigquery.Table(table_ref, schema=schema)\n",
    "client.create_table(table)\n",
    "\n",
    "# Insertar los datos en la tabla\n",
    "rows = []\n",
    "for feature in data['features']:\n",
    "    properties = feature['properties']\n",
    "    time_str = properties['time']\n",
    "    time = pd.to_datetime(time_str, unit= \"ms\") #parse(time_str)\n",
    "    row = (\n",
    "        properties['ids'],\n",
    "        properties['mag'],\n",
    "        properties['place'],\n",
    "        time,\n",
    "        properties['url']\n",
    "        # Ajusta los índices de las coordenadas si es necesario\n",
    "    )\n",
    "    rows.append(row)\n",
    "\n",
    "client.insert_rows(table, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizamos la data de google query \n",
    "\n",
    "import requests\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismo'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Colombia2'\n",
    "\n",
    "# Crear el cliente de BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "latest_date_query = f\"SELECT MAX(time) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "query_job = client.query(latest_date_query)\n",
    "latest_date_result = query_job.result()\n",
    "\n",
    "# Obtener el valor de la fecha más reciente\n",
    "for row in latest_date_result:\n",
    "    latest_date = row[0]\n",
    "\n",
    "# Verificar si se obtuvo una fecha válida\n",
    "if latest_date is not None:\n",
    "    latest_date = latest_date.date()\n",
    "else:\n",
    "    # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "    latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "# Obtener la fecha actual\n",
    "current_date = datetime.now().date()\n",
    "\n",
    "# Verificar si se necesita actualizar la tabla\n",
    "if current_date > latest_date:\n",
    "    # Calcular la fecha de inicio y fin para la API\n",
    "    start_date = latest_date + timedelta(days=1)\n",
    "    end_date = current_date\n",
    "\n",
    "    # Obtener los datos de la API de USGS\n",
    "    url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "    parameters = {\n",
    "        'format': 'geojson',\n",
    "        'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "        'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "        'minmagnitude': '2.5',\n",
    "        'minlatitude': '-5.266',\n",
    "        'maxlatitude': '15.708',\n",
    "        'minlongitude': '276.328',\n",
    "        'maxlongitude': '293.906',\n",
    "        'limit': 20000\n",
    "    }\n",
    "    response = requests.get(url, params=parameters)\n",
    "    data = response.json()\n",
    "\n",
    "    # Insertar los datos en la tabla\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    rows = []\n",
    "    for feature in data['features']:\n",
    "        properties = feature['properties']\n",
    "        time_str = properties['time']\n",
    "        time = pd.to_datetime(time_str, unit=\"ms\")\n",
    "        row = (\n",
    "            properties['ids'],\n",
    "            properties['mag'],\n",
    "            properties['place'],\n",
    "            time,\n",
    "            properties['url']\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "    client.insert_rows(table, rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OJO: en el archivo de requerimientos.txt de cloud functions se completa con \n",
    "google-cloud-bigquery\n",
    "requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "como puedo hacer esta misma tarea, pero en google cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar esta tarea en Google Cloud, puedes utilizar Cloud Functions y Cloud Scheduler. Cloud Functions te permite ejecutar código de forma escalable y sin preocuparte por la infraestructura subyacente, mientras que Cloud Scheduler te permite programar la ejecución de funciones en un horario específico.\n",
    "\n",
    "Aquí tienes los pasos a seguir para configurar esta tarea en Google Cloud:\n",
    "\n",
    "Crea una función en Cloud Functions:\n",
    "\n",
    "Ve a la página de Cloud Functions en la Consola de Google Cloud.\n",
    "Haz clic en \"Crear función\".\n",
    "Asigna un nombre a la función, por ejemplo, \"actualizar_tabla_sismos\".\n",
    "Selecciona el entorno de ejecución \"Python\".\n",
    "Copia y pega el código que te proporcioné anteriormente en el editor de código.\n",
    "Asegúrate de que el código incluya las importaciones necesarias y la configuración de autenticación para BigQuery.\n",
    "Define la función llamada actualizar_tabla_sismos(request), que es el punto de entrada para la función.\n",
    "Guarda los cambios y crea la función.\n",
    "Crea un trabajo en Cloud Scheduler:\n",
    "\n",
    "Ve a la página de Cloud Scheduler en la Consola de Google Cloud.\n",
    "Haz clic en \"Crear trabajo\".\n",
    "Asigna un nombre al trabajo, por ejemplo, \"actualizar_tabla_sismos_job\".\n",
    "Configura el intervalo para que se ejecute diariamente a las 13:00 en la zona horaria de Colombia. Puedes usar la expresión 0 13 * * * en el campo \"Frecuencia\".\n",
    "En \"Tipo de destino\", selecciona \"Cloud Pub/Sub\".\n",
    "Crea o selecciona un tema de Pub/Sub existente.\n",
    "Haz clic en \"Crear\".\n",
    "Configura la función para ser invocada por Cloud Scheduler:\n",
    "\n",
    "Ve a la página de Cloud Functions en la Consola de Google Cloud.\n",
    "Haz clic en el nombre de la función que creaste anteriormente.\n",
    "En la pestaña \"Disparadores\", haz clic en \"Agregar disparador\".\n",
    "Selecciona \"Cloud Pub/Sub\" como tipo de disparador.\n",
    "Elige el tema de Pub/Sub que configuraste en el paso anterior.\n",
    "Guarda los cambios.\n",
    "Ahora, Cloud Scheduler ejecutará tu función automáticamente todos los días a las 13:00 en la zona horaria de Colombia. El código de la función verificará si es necesario actualizar la tabla \"Colombia\" en BigQuery y realizará la actualización si es necesario.\n",
    "\n",
    "Asegúrate de configurar correctamente las credenciales y los permisos necesarios para acceder a BigQuery y la API de USGS en tu entorno de ejecución de Google Cloud Functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar un Cloud Scheduler existente a una Cloud Function en Google Cloud Platform, sigue estos pasos:\n",
    "\n",
    "Asegúrate de que ya tienes tanto la Cloud Function como el Cloud Scheduler creados en tu proyecto de Google Cloud Platform.\n",
    "\n",
    "Abre la consola de Cloud Console y selecciona tu proyecto.\n",
    "\n",
    "Navega hasta la página de la función que deseas asociar con el Cloud Scheduler. Puedes encontrar tus funciones en la sección \"Cloud Functions\" en el menú de servicios.\n",
    "\n",
    "Haz clic en la función que deseas editar para abrir su configuración.\n",
    "\n",
    "En la página de configuración de la función, busca la sección \"Triggers\" (Desencadenadores) y haz clic en \"Add trigger\" (Agregar desencadenador).\n",
    "\n",
    "En el menú desplegable de tipos de desencadenadores, selecciona \"Cloud Pub/Sub\".\n",
    "\n",
    "Configura el desencadenador de Pub/Sub con los detalles necesarios. En el campo \"Topic\", elige el tema de Pub/Sub asociado al Cloud Scheduler que deseas usar. También puedes especificar un identificador único para el desencadenador si lo deseas.\n",
    "\n",
    "Haz clic en \"Guardar\" para guardar los cambios en la configuración de la función.\n",
    "\n",
    "Ahora, cuando el Cloud Scheduler active el Cloud Pub/Sub, se desencadenará la función asociada y se ejecutará según lo programado.\n",
    "\n",
    "Recuerda que debes configurar adecuadamente el Cloud Scheduler y el Cloud Pub/Sub para que se ejecuten en los horarios y las condiciones deseadas. También asegúrate de que la función se haya implementado correctamente y esté lista para manejar las solicitudes cuando se active a través del Cloud Scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue la función que se implemento e google cloud functions \n",
    "\n",
    "import functions_framework\n",
    "import requests\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@functions_framework.http\n",
    "def hello_http(request):\n",
    "    # Configurar el proyecto, dataset y tabla\n",
    "    project_id = 'henry-sismo'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'Colombia'\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(time) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '-5.266',\n",
    "            'maxlatitude': '15.708',\n",
    "            'minlongitude': '276.328',\n",
    "            'maxlongitude': '293.906',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        # Insertar los datos en la tabla\n",
    "        table_ref = client.dataset(dataset_id).table(table_id)\n",
    "        table = client.get_table(table_ref)\n",
    "\n",
    "        rows = []\n",
    "        for feature in data['features']:\n",
    "            properties = feature['properties']\n",
    "            time_str = properties['time']\n",
    "            time = datetime.fromtimestamp(int(time_str) / 1000)  # Convertir a datetime\n",
    "            row = (\n",
    "                properties['ids'],\n",
    "                properties['mag'],\n",
    "                properties['place'],\n",
    "                time,\n",
    "                properties['url']\n",
    "            )\n",
    "            rows.append(row)\n",
    "\n",
    "        client.insert_rows(table, rows)\n",
    "\n",
    "    return 'Data actualizada correctamente'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y estos el contenido del requirements.txt\n",
    "\n",
    "functions-framework==3.*\n",
    "requests\n",
    "google-cloud-bigquery"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
