{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datetime\n",
    "#from dateutil.parser import parse\n",
    "\n",
    "#from datetime import datetime, timedelta\n",
    "\n",
    "#from google.api_core.exceptions import Conflict\n",
    "\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "# Creamos los permisos con la llave para ingresar a Google \n",
    "# activamos el servicio\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../notebooks/Clave_Google/henry-sismos-a343182ba163.json\"\n",
    "\n",
    "project_id = 'your-project-id'\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-06-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '-5.266',\n",
    "    'maxlatitude': '15.708',\n",
    "    'minlongitude': '276.328',\n",
    "    'maxlongitude': '293.906',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Colombia'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField('id', 'STRING'),\n",
    "    bigquery.SchemaField('magnitude', 'FLOAT'),\n",
    "    bigquery.SchemaField('place', 'STRING'),\n",
    "    bigquery.SchemaField('time', 'TIMESTAMP'),\n",
    "    bigquery.SchemaField('url', 'STRING')\n",
    "    # Agrega más campos según los datos que desees almacenar\n",
    "]\n",
    "\n",
    "table = bigquery.Table(table_ref, schema=schema)\n",
    "client.create_table(table)\n",
    "\n",
    "# Insertar los datos en la tabla\n",
    "rows = []\n",
    "for feature in data['features']:\n",
    "    properties = feature['properties']\n",
    "    time_str = properties['time']\n",
    "    time = pd.to_datetime(time_str, unit= \"ms\") #parse(time_str)\n",
    "    row = (\n",
    "        properties['ids'],\n",
    "        properties['mag'],\n",
    "        properties['place'],\n",
    "        time,\n",
    "        properties['url']\n",
    "        # Ajusta los índices de las coordenadas si es necesario\n",
    "    )\n",
    "    rows.append(row)\n",
    "\n",
    "client.insert_rows(table, rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actualizamos la data de google query\n",
    "\n",
    "import requests\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismo'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Colombia'\n",
    "\n",
    "# Crear el cliente de BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "latest_date_query = f\"SELECT MAX(time) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "query_job = client.query(latest_date_query)\n",
    "latest_date_result = query_job.result()\n",
    "\n",
    "# Obtener el valor de la fecha más reciente\n",
    "for row in latest_date_result:\n",
    "    latest_date = row[0]\n",
    "\n",
    "# Verificar si se obtuvo una fecha válida\n",
    "if latest_date is not None:\n",
    "    latest_date = latest_date.date()\n",
    "else:\n",
    "    # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "    latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "# Obtener la fecha actual\n",
    "current_date = datetime.now().date()\n",
    "\n",
    "# Verificar si se necesita actualizar la tabla\n",
    "if current_date > latest_date:\n",
    "    # Calcular la fecha de inicio y fin para la API\n",
    "    start_date = latest_date + timedelta(days=1)\n",
    "    end_date = current_date\n",
    "\n",
    "    # Obtener los datos de la API de USGS\n",
    "    url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "    parameters = {\n",
    "        'format': 'geojson',\n",
    "        'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "        'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "        'minmagnitude': '2.5',\n",
    "        'minlatitude': '-5.266',\n",
    "        'maxlatitude': '15.708',\n",
    "        'minlongitude': '276.328',\n",
    "        'maxlongitude': '293.906',\n",
    "        'limit': 20000\n",
    "    }\n",
    "    response = requests.get(url, params=parameters)\n",
    "    data = response.json()\n",
    "\n",
    "    # Insertar los datos en la tabla si hay nuevos datos\n",
    "    if 'features' in data and len(data['features']) > 0:\n",
    "        table_ref = client.dataset(dataset_id).table(table_id)\n",
    "        table = client.get_table(table_ref)\n",
    "\n",
    "        rows = []\n",
    "        for feature in data['features']:\n",
    "            properties = feature['properties']\n",
    "            time_str = properties['time']\n",
    "            time = pd.to_datetime(time_str, unit=\"ms\")\n",
    "            row = (\n",
    "                properties['ids'],\n",
    "                properties['mag'],\n",
    "                properties['place'],\n",
    "                time,\n",
    "                properties['url']\n",
    "            )\n",
    "            rows.append(row)\n",
    "\n",
    "        client.insert_rows(table, rows)\n",
    "    else:\n",
    "        print(\"No hay nuevos datos para insertar.\")\n",
    "else:\n",
    "    print(\"La tabla ya está actualizada.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "como puedo hacer esta misma tarea, pero en google cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar esta tarea en Google Cloud, puedes utilizar Cloud Functions y Cloud Scheduler. Cloud Functions te permite ejecutar código de forma escalable y sin preocuparte por la infraestructura subyacente, mientras que Cloud Scheduler te permite programar la ejecución de funciones en un horario específico.\n",
    "\n",
    "Aquí tienes los pasos a seguir para configurar esta tarea en Google Cloud:\n",
    "\n",
    "Crea una función en Cloud Functions:\n",
    "\n",
    "Ve a la página de Cloud Functions en la Consola de Google Cloud.\n",
    "Haz clic en \"Crear función\".\n",
    "Asigna un nombre a la función, por ejemplo, \"actualizar_tabla_sismos\".\n",
    "Selecciona el entorno de ejecución \"Python\".\n",
    "Copia y pega el código que te proporcioné anteriormente en el editor de código.\n",
    "Asegúrate de que el código incluya las importaciones necesarias y la configuración de autenticación para BigQuery.\n",
    "Define la función llamada actualizar_tabla_sismos(request), que es el punto de entrada para la función.\n",
    "Guarda los cambios y crea la función.\n",
    "Crea un trabajo en Cloud Scheduler:\n",
    "\n",
    "Ve a la página de Cloud Scheduler en la Consola de Google Cloud.\n",
    "Haz clic en \"Crear trabajo\".\n",
    "Asigna un nombre al trabajo, por ejemplo, \"actualizar_tabla_sismos_job\".\n",
    "Configura el intervalo para que se ejecute diariamente a las 13:00 en la zona horaria de Colombia. Puedes usar la expresión 0 13 * * * en el campo \"Frecuencia\".\n",
    "En \"Tipo de destino\", selecciona \"Cloud Pub/Sub\".\n",
    "Crea o selecciona un tema de Pub/Sub existente.\n",
    "Haz clic en \"Crear\".\n",
    "Configura la función para ser invocada por Cloud Scheduler:\n",
    "\n",
    "Ve a la página de Cloud Functions en la Consola de Google Cloud.\n",
    "Haz clic en el nombre de la función que creaste anteriormente.\n",
    "En la pestaña \"Disparadores\", haz clic en \"Agregar disparador\".\n",
    "Selecciona \"Cloud Pub/Sub\" como tipo de disparador.\n",
    "Elige el tema de Pub/Sub que configuraste en el paso anterior.\n",
    "Guarda los cambios.\n",
    "Ahora, Cloud Scheduler ejecutará tu función automáticamente todos los días a las 13:00 en la zona horaria de Colombia. El código de la función verificará si es necesario actualizar la tabla \"Colombia\" en BigQuery y realizará la actualización si es necesario.\n",
    "\n",
    "Asegúrate de configurar correctamente las credenciales y los permisos necesarios para acceder a BigQuery y la API de USGS en tu entorno de ejecución de Google Cloud Functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar un Cloud Scheduler existente a una Cloud Function en Google Cloud Platform, sigue estos pasos:\n",
    "\n",
    "Asegúrate de que ya tienes tanto la Cloud Function como el Cloud Scheduler creados en tu proyecto de Google Cloud Platform.\n",
    "\n",
    "Abre la consola de Cloud Console y selecciona tu proyecto.\n",
    "\n",
    "Navega hasta la página de la función que deseas asociar con el Cloud Scheduler. Puedes encontrar tus funciones en la sección \"Cloud Functions\" en el menú de servicios.\n",
    "\n",
    "Haz clic en la función que deseas editar para abrir su configuración.\n",
    "\n",
    "En la página de configuración de la función, busca la sección \"Triggers\" (Desencadenadores) y haz clic en \"Add trigger\" (Agregar desencadenador).\n",
    "\n",
    "En el menú desplegable de tipos de desencadenadores, selecciona \"Cloud Pub/Sub\".\n",
    "\n",
    "Configura el desencadenador de Pub/Sub con los detalles necesarios. En el campo \"Topic\", elige el tema de Pub/Sub asociado al Cloud Scheduler que deseas usar. También puedes especificar un identificador único para el desencadenador si lo deseas.\n",
    "\n",
    "Haz clic en \"Guardar\" para guardar los cambios en la configuración de la función.\n",
    "\n",
    "Ahora, cuando el Cloud Scheduler active el Cloud Pub/Sub, se desencadenará la función asociada y se ejecutará según lo programado.\n",
    "\n",
    "Recuerda que debes configurar adecuadamente el Cloud Scheduler y el Cloud Pub/Sub para que se ejecuten en los horarios y las condiciones deseadas. También asegúrate de que la función se haya implementado correctamente y esté lista para manejar las solicitudes cuando se active a través del Cloud Scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esta fue la función que se implemento e google cloud functions \n",
    "\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "def actualizar_colombia(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'Colombia'\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    #client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(time) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '-5.266',\n",
    "            'maxlatitude': '15.708',\n",
    "            'minlongitude': '276.328',\n",
    "            'maxlongitude': '293.906',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        # Insertar los datos en la tabla si hay nuevos datos\n",
    "        if 'features' in data and len(data['features']) > 0:\n",
    "            table_ref = client.dataset(dataset_id).table(table_id)\n",
    "            table = client.get_table(table_ref)\n",
    "\n",
    "            rows = []\n",
    "            for feature in data['features']:\n",
    "                properties = feature['properties']\n",
    "                time_str = properties['time']\n",
    "                time = pd.to_datetime(time_str, unit=\"ms\")\n",
    "                row = (\n",
    "                    properties['ids'],\n",
    "                    properties['mag'],\n",
    "                    properties['place'],\n",
    "                    time,\n",
    "                    properties['url']\n",
    "                )\n",
    "                rows.append(row)\n",
    "\n",
    "            client.insert_rows(table, rows)\n",
    "        else:\n",
    "            \"No hay nuevos datos para insertar.\"\n",
    "    else:\n",
    "        \"La tabla ya está actualizada.\"\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y estos el contenido del requirements.txt<br>\n",
    "\n",
    "google-cloud-bigquery\n",
    "requests\n",
    "pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos la ETL por Pais para google cloud\n",
    "\n",
    "Ya teniendo la conexión y un código funcional, es el momento de aplicar la ETL ya desarrollada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colombia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n",
      "La tabla Colombia_sismo se ha creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '-5.266',\n",
    "    'maxlatitude': '15.708',\n",
    "    'minlongitude': '276.328',\n",
    "    'maxlongitude': '293.906',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    latitude = event['geometry']['coordinates'][1]\n",
    "    longitude = event['geometry']['coordinates'][0]\n",
    "    records.append(properties)\n",
    "\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_Colombia = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit= \"ms\")\n",
    "# Quitemos los nulos\n",
    "df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "df_Colombia = df_Colombia.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_Colombia['Region'] = prefectures\n",
    "df_Colombia['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['CO']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\", 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_Colombia.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_Colombia\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Colombia_sismo'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue la función que se implemento e google cloud functions carga incremental o actualizacion de informacion\n",
    "\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_colombia_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'Colombia_sismo'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '-5.266',\n",
    "            'maxlatitude': '15.708',\n",
    "            'minlongitude': '276.328',\n",
    "            'maxlongitude': '293.906',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_Colombia = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "        \n",
    "        df_Colombia = df_Colombia.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_Colombia.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_Colombia['latitude']) == len(df_Colombia['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_Colombia['Region'] = regions\n",
    "                df_Colombia['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['CO']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\",\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_Colombia.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_Colombia.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Lugar\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Id\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Region\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Pais\", \"STRING\")\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_Colombia está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt\n",
    "google-cloud-bigquery <br>\n",
    "pandas <br>\n",
    "reverse_geocoder <br>\n",
    "requests <br>\n",
    "pyarrow <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla Japan_sismo se ha creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '28.691',\n",
    "    'maxlatitude': '47.458',\n",
    "    'minlongitude': '125.859',\n",
    "    'maxlongitude': '156.445',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    latitude = event['geometry']['coordinates'][1]\n",
    "    longitude = event['geometry']['coordinates'][0]\n",
    "    records.append(properties)\n",
    "\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_Colombia = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit= \"ms\")\n",
    "# Quitemos los nulos\n",
    "df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "df_Colombia = df_Colombia.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_Colombia['Region'] = prefectures\n",
    "df_Colombia['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['JP']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\", 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_Colombia.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_Colombia\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Japan_sismo'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue la función que se implemento e google cloud functions carga incremental o actualizacion de informacion\n",
    "\n",
    "\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_japan_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'Japan_sismo'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '28.691',\n",
    "            'maxlatitude': '47.458',\n",
    "            'minlongitude': '125.859',\n",
    "            'maxlongitude': '156.445',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_Colombia = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "        \n",
    "        df_Colombia = df_Colombia.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_Colombia.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_Colombia['latitude']) == len(df_Colombia['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_Colombia['Region'] = regions\n",
    "                df_Colombia['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['JP']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\",\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_Colombia.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_Colombia.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Lugar\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Id\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Region\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Pais\", \"STRING\")\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_Colombia está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt\n",
    "google-cloud-bigquery <br>\n",
    "pandas <br>\n",
    "reverse_geocoder <br>\n",
    "requests <br>\n",
    "pyarrow <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEUU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla EEUU_sismo se ha creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '12.555',\n",
    "    'maxlatitude': '75.141',\n",
    "    'minlongitude': '182.813',\n",
    "    'maxlongitude': '315',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    latitude = event['geometry']['coordinates'][1]\n",
    "    longitude = event['geometry']['coordinates'][0]\n",
    "    records.append(properties)\n",
    "\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_Colombia = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit= \"ms\")\n",
    "# Quitemos los nulos\n",
    "df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "df_Colombia = df_Colombia.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_Colombia['Region'] = prefectures\n",
    "df_Colombia['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['US']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\", 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_Colombia.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_Colombia\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'EEUU_sismo'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue la función que se implemento e google cloud functions carga incremental o actualizacion de informacion\n",
    "\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_EEUU_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'EEUU_sismo'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '12.555',\n",
    "            'maxlatitude': '75.141',\n",
    "            'minlongitude': '182.813',\n",
    "            'maxlongitude': '315',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_Colombia = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\"]]\n",
    "        \n",
    "        df_Colombia = df_Colombia.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_Colombia.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_Colombia['latitude']) == len(df_Colombia['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_Colombia['Region'] = regions\n",
    "                df_Colombia['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['US']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\",\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_Colombia.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_Colombia.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Lugar\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Id\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Region\", \"STRING\"),\n",
    "                        bigquery.SchemaField(\"Pais\", \"STRING\")\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_Colombia está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
