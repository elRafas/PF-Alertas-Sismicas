{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datetime\n",
    "#from dateutil.parser import parse\n",
    "\n",
    "#from datetime import datetime, timedelta\n",
    "\n",
    "#from google.api_core.exceptions import Conflict\n",
    "\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "from datetime import date\n",
    "\n",
    "# Creamos los permisos con la llave para ingresar a Google \n",
    "# activamos el servicio\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../../notebooks/Clave_Google/henry-sismos-a343182ba163.json\"\n",
    "\n",
    "project_id = 'your-project-id'\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colombia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 391 entries, 2 to 3692\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   Fecha       391 non-null    datetime64[ns]\n",
      " 1   Magnitud    391 non-null    float64       \n",
      " 2   Intensidad  391 non-null    float64       \n",
      " 3   GAP         391 non-null    float64       \n",
      " 4   Latitud     391 non-null    float64       \n",
      " 5   Longitud    391 non-null    float64       \n",
      " 6   Lugar       391 non-null    object        \n",
      " 7   Id          391 non-null    object        \n",
      " 8   Hipocentro  391 non-null    float64       \n",
      " 9   Region      391 non-null    object        \n",
      " 10  Pais        391 non-null    object        \n",
      " 11  Act         391 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(6), object(4)\n",
      "memory usage: 39.7+ KB\n",
      "La tabla Colombia_sismo_Dashboards ya existe.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '4.0',\n",
    "    'minlatitude': '-5.266',\n",
    "    'maxlatitude': '15.708',\n",
    "    'minlongitude': '276.328',\n",
    "    'maxlongitude': '293.906',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "\n",
    "    \n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    properties['hipocentro'] = coordinates[2]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_Colombia = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit= \"ms\")\n",
    "\n",
    "\n",
    "# Quitemos los nulos\n",
    "df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\" ]]\n",
    "df_Colombia = df_Colombia.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_Colombia['Region'] = prefectures\n",
    "df_Colombia['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['CO']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\",\n",
    "                   'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", \n",
    "                   'place': \"Lugar\", 'ids': \"Id\", \"hipocentro\": 'Hipocentro', 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_Colombia.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos la columna de la actualización de la data \n",
    "df_Colombia[\"Act\"] = pd.to_datetime(date.today()) \n",
    "\n",
    "df_Colombia.info()\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_Colombia\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Colombia_sismo_Dashboards'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING'),\n",
    "        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 395 entries, 2 to 4002\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   Fecha       395 non-null    datetime64[ns]\n",
      " 1   Magnitud    395 non-null    float64       \n",
      " 2   Intensidad  395 non-null    float64       \n",
      " 3   GAP         395 non-null    float64       \n",
      " 4   Latitud     395 non-null    float64       \n",
      " 5   Longitud    395 non-null    float64       \n",
      " 6   Lugar       395 non-null    object        \n",
      " 7   Id          395 non-null    object        \n",
      " 8   Hipocentro  395 non-null    float64       \n",
      " 9   Region      395 non-null    object        \n",
      " 10  Pais        395 non-null    object        \n",
      " 11  Act         395 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(6), object(4)\n",
      "memory usage: 40.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_Colombia.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este es el código que se coloca en Google BigQuery\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_colombia_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'Colombia_sismo'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '-5.266',\n",
    "            'maxlatitude': '15.708',\n",
    "            'minlongitude': '276.328',\n",
    "            'maxlongitude': '293.906',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            longitude2 = event['geometry']['coordinates'][2]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            properties['hipocentro'] = longitude2\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_Colombia = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\"]]\n",
    "        \n",
    "        df_Colombia = df_Colombia.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_Colombia.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_Colombia['latitude']) == len(df_Colombia['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_Colombia['Region'] = regions\n",
    "                df_Colombia['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['CO']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\" , \"hipocentro\": 'Hipocentro',\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_Colombia.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Creamos la columna de la actualización de la data \n",
    "                df_Colombia[\"Act\"] = pd.to_datetime(date.today()) \n",
    "\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_Colombia.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "                        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "                        bigquery.SchemaField('Id', 'STRING'),\n",
    "                        bigquery.SchemaField('Region', 'STRING'),\n",
    "                        bigquery.SchemaField('Pais', 'STRING'),\n",
    "                        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_Colombia está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt\n",
    "google-cloud-bigquery <br>\n",
    "pandas <br>\n",
    "reverse_geocoder <br>\n",
    "requests <br>\n",
    "pyarrow <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla Japan_sismo_Dashboards se ha creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '28.691',\n",
    "    'maxlatitude': '47.458',\n",
    "    'minlongitude': '125.859',\n",
    "    'maxlongitude': '156.445',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "\n",
    "\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    properties['hipocentro'] = coordinates[2]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_Japan = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_Japan[\"time\"] = pd.to_datetime(df_Japan[\"time\"], unit= \"ms\")\n",
    "\n",
    "\n",
    "# Quitemos los nulos\n",
    "df_Japan = df_Japan[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\" ]]\n",
    "df_Japan = df_Japan.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_Japan['latitude'], df_Japan['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_Japan['Region'] = prefectures\n",
    "df_Japan['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['JP']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_Japan = df_Japan[df_Japan['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\",\n",
    "                   'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", \n",
    "                   'place': \"Lugar\", 'ids': \"Id\", \"hipocentro\": 'Hipocentro', 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_Japan.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos la columna de la actualización de la data \n",
    "df_Japan[\"Act\"] = pd.to_datetime(date.today()) \n",
    "\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_Japan\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Japan_sismo_Dashboards'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING'),\n",
    "        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_Japan, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue la función que se implemento e google cloud functions carga incremental o actualizacion de informacion\n",
    "\n",
    "# Este es el código que se coloca en Google BigQuery\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_japan_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'Japan_sismo_'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '28.691',\n",
    "            'maxlatitude': '47.458',\n",
    "            'minlongitude': '125.859',\n",
    "            'maxlongitude': '156.445',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            longitude2 = event['geometry']['coordinates'][2]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            properties['hipocentro'] = longitude2\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_Japan = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_Japan[\"time\"] = pd.to_datetime(df_Japan[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_Japan = df_Japan[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\"]]\n",
    "        \n",
    "        df_Japan = df_Japan.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_Japan.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_Japan['latitude']) == len(df_Japan['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_Japan['latitude'], df_Japan['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_Japan['Region'] = regions\n",
    "                df_Japan['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['JP']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_Japan = df_Japan[df_Japan['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\" , \"hipocentro\": 'Hipocentro',\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_Japan.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Creamos la columna de la actualización de la data \n",
    "                df_Japan[\"Act\"] = pd.to_datetime(date.today()) \n",
    "\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_Japan.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "                        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "                        bigquery.SchemaField('Id', 'STRING'),\n",
    "                        bigquery.SchemaField('Region', 'STRING'),\n",
    "                        bigquery.SchemaField('Pais', 'STRING'),\n",
    "                        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_Japan, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_Japan está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt\n",
    "google-cloud-bigquery <br>\n",
    "pandas <br>\n",
    "reverse_geocoder <br>\n",
    "requests <br>\n",
    "pyarrow <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEUU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3243 entries, 15 to 19988\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   Fecha       3243 non-null   datetime64[ns]\n",
      " 1   Magnitud    3243 non-null   float64       \n",
      " 2   Intensidad  3243 non-null   float64       \n",
      " 3   GAP         3243 non-null   float64       \n",
      " 4   Latitud     3243 non-null   float64       \n",
      " 5   Longitud    3243 non-null   float64       \n",
      " 6   Lugar       3243 non-null   object        \n",
      " 7   Id          3243 non-null   object        \n",
      " 8   Hipocentro  3243 non-null   float64       \n",
      " 9   Region      3243 non-null   object        \n",
      " 10  Pais        3243 non-null   object        \n",
      " 11  Act         3243 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(6), object(4)\n",
      "memory usage: 329.4+ KB\n",
      "La tabla EEUU_sismo_Dashboards ya existe.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "\n",
    "\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '12.555',\n",
    "    'maxlatitude': '75.141',\n",
    "    'minlongitude': '182.813',\n",
    "    'maxlongitude': '315',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    properties['hipocentro'] = coordinates[2]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_EEUU = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_EEUU[\"time\"] = pd.to_datetime(df_EEUU[\"time\"], unit= \"ms\")\n",
    "\n",
    "\n",
    "\n",
    "# Quitemos los nulos\n",
    "df_EEUU = df_EEUU[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\" ]]\n",
    "df_EEUU = df_EEUU.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_EEUU['latitude'], df_EEUU['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_EEUU['Region'] = prefectures\n",
    "df_EEUU['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['US']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_EEUU = df_EEUU[df_EEUU['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\",\n",
    "                   'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", \n",
    "                   'place': \"Lugar\", 'ids': \"Id\", \"hipocentro\": 'Hipocentro', 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_EEUU.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos la columna de la actualización de la data \n",
    "df_EEUU[\"Act\"] = pd.to_datetime(date.today()) \n",
    "df_EEUU.info()\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_EEUU\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'EEUU_sismo_Dashboards'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING'),\n",
    "        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_EEUU, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue la función que se implemento e google cloud functions carga incremental o actualizacion de informacion\n",
    "\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_EEUU_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'EEUU_sismo'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '12.555',\n",
    "            'maxlatitude': '75.141',\n",
    "            'minlongitude': '182.813',\n",
    "            'maxlongitude': '315',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            longitude2 = event['geometry']['coordinates'][2]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            properties['hipocentro'] = longitude2\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_EEUU = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_EEUU[\"time\"] = pd.to_datetime(df_EEUU[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_EEUU = df_EEUU[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\"]]\n",
    "        \n",
    "        df_EEUU = df_EEUU.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_EEUU.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_EEUU['latitude']) == len(df_EEUU['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_EEUU['latitude'], df_EEUU['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_EEUU['Region'] = regions\n",
    "                df_EEUU['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['US']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_EEUU = df_EEUU[df_EEUU['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\" , \"hipocentro\": 'Hipocentro',\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_EEUU.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Creamos la columna de la actualización de la data \n",
    "                df_EEUU[\"Act\"] = pd.to_datetime(date.today()) \n",
    "\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_EEUU.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "                        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "                        bigquery.SchemaField('Id', 'STRING'),\n",
    "                        bigquery.SchemaField('Region', 'STRING'),\n",
    "                        bigquery.SchemaField('Pais', 'STRING'),\n",
    "                        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_EEUU, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_EEUU está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
