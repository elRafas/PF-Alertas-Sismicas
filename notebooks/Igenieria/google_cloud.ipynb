{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datetime\n",
    "#from dateutil.parser import parse\n",
    "\n",
    "#from datetime import datetime, timedelta\n",
    "\n",
    "#from google.api_core.exceptions import Conflict\n",
    "\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "from datetime import date\n",
    "\n",
    "# Creamos los permisos con la llave para ingresar a Google \n",
    "# activamos el servicio\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../../notebooks/Clave_Google/henry-sismos-a343182ba163.json\"\n",
    "\n",
    "project_id = 'your-project-id'\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query para contar los valoree por cada actualizacion\n",
    "SELECT Act, COUNT(Fecha) AS CountFecha\n",
    "FROM `henry-sismos.Sismos.EEUU_sismo`\n",
    "GROUP BY Act"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos la ETL por Pais para google cloud\n",
    "\n",
    "Ya teniendo la conexión y un código funcional, es el momento de aplicar la ETL ya desarrollada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colombia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla Colombia_sismo ya existe.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '-5.266',\n",
    "    'maxlatitude': '15.708',\n",
    "    'minlongitude': '276.328',\n",
    "    'maxlongitude': '293.906',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "\n",
    "    \n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    properties['hipocentro'] = coordinates[2]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_Colombia = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit= \"ms\")\n",
    "\n",
    "\n",
    "# Quitemos los nulos\n",
    "df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\" ]]\n",
    "df_Colombia = df_Colombia.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_Colombia['Region'] = prefectures\n",
    "df_Colombia['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['CO']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\",\n",
    "                   'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", \n",
    "                   'place': \"Lugar\", 'ids': \"Id\", \"hipocentro\": 'Hipocentro', 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_Colombia.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos la columna de la actualización de la data \n",
    "df_Colombia[\"Act\"] = pd.to_datetime(date.today()) \n",
    "\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_Colombia\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Colombia_sismo'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING'),\n",
    "        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 395 entries, 2 to 4002\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   Fecha       395 non-null    datetime64[ns]\n",
      " 1   Magnitud    395 non-null    float64       \n",
      " 2   Intensidad  395 non-null    float64       \n",
      " 3   GAP         395 non-null    float64       \n",
      " 4   Latitud     395 non-null    float64       \n",
      " 5   Longitud    395 non-null    float64       \n",
      " 6   Lugar       395 non-null    object        \n",
      " 7   Id          395 non-null    object        \n",
      " 8   Hipocentro  395 non-null    float64       \n",
      " 9   Region      395 non-null    object        \n",
      " 10  Pais        395 non-null    object        \n",
      " 11  Act         395 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(6), object(4)\n",
      "memory usage: 40.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_Colombia.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este es el código que se coloca en Google BigQuery\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_colombia_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'Colombia_sismo'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '-5.266',\n",
    "            'maxlatitude': '15.708',\n",
    "            'minlongitude': '276.328',\n",
    "            'maxlongitude': '293.906',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            longitude2 = event['geometry']['coordinates'][2]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            properties['hipocentro'] = longitude2\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_Colombia = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_Colombia[\"time\"] = pd.to_datetime(df_Colombia[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_Colombia = df_Colombia[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\"]]\n",
    "        \n",
    "        df_Colombia = df_Colombia.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_Colombia.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_Colombia['latitude']) == len(df_Colombia['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_Colombia['latitude'], df_Colombia['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_Colombia['Region'] = regions\n",
    "                df_Colombia['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['CO']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_Colombia = df_Colombia[df_Colombia['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\" , \"hipocentro\": 'Hipocentro',\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_Colombia.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Creamos la columna de la actualización de la data \n",
    "                df_Colombia[\"Act\"] = pd.to_datetime(date.today()) \n",
    "\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_Colombia.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "                        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "                        bigquery.SchemaField('Id', 'STRING'),\n",
    "                        bigquery.SchemaField('Region', 'STRING'),\n",
    "                        bigquery.SchemaField('Pais', 'STRING'),\n",
    "                        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_Colombia, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_Colombia está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt\n",
    "google-cloud-bigquery <br>\n",
    "pandas <br>\n",
    "reverse_geocoder <br>\n",
    "requests <br>\n",
    "pyarrow <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla Japan_sismo ya existe.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '28.691',\n",
    "    'maxlatitude': '47.458',\n",
    "    'minlongitude': '125.859',\n",
    "    'maxlongitude': '156.445',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "\n",
    "\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    properties['hipocentro'] = coordinates[2]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_Japan = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_Japan[\"time\"] = pd.to_datetime(df_Japan[\"time\"], unit= \"ms\")\n",
    "\n",
    "\n",
    "# Quitemos los nulos\n",
    "df_Japan = df_Japan[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\" ]]\n",
    "df_Japan = df_Japan.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_Japan['latitude'], df_Japan['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_Japan['Region'] = prefectures\n",
    "df_Japan['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['JP']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_Japan = df_Japan[df_Japan['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\",\n",
    "                   'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", \n",
    "                   'place': \"Lugar\", 'ids': \"Id\", \"hipocentro\": 'Hipocentro', 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_Japan.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos la columna de la actualización de la data \n",
    "df_Japan[\"Act\"] = pd.to_datetime(date.today()) \n",
    "\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_Japan\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'Japan_sismo'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING'),\n",
    "        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_Japan, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue la función que se implemento e google cloud functions carga incremental o actualizacion de informacion\n",
    "\n",
    "# Este es el código que se coloca en Google BigQuery\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_japan_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'Japan_sismo'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '28.691',\n",
    "            'maxlatitude': '47.458',\n",
    "            'minlongitude': '125.859',\n",
    "            'maxlongitude': '156.445',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            longitude2 = event['geometry']['coordinates'][2]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            properties['hipocentro'] = longitude2\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_Japan = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_Japan[\"time\"] = pd.to_datetime(df_Japan[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_Japan = df_Japan[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\"]]\n",
    "        \n",
    "        df_Japan = df_Japan.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_Japan.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_Japan['latitude']) == len(df_Japan['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_Japan['latitude'], df_Japan['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_Japan['Region'] = regions\n",
    "                df_Japan['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['JP']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_Japan = df_Japan[df_Japan['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\" , \"hipocentro\": 'Hipocentro',\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_Japan.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Creamos la columna de la actualización de la data \n",
    "                df_Japan[\"Act\"] = pd.to_datetime(date.today()) \n",
    "\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_Japan.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "                        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "                        bigquery.SchemaField('Id', 'STRING'),\n",
    "                        bigquery.SchemaField('Region', 'STRING'),\n",
    "                        bigquery.SchemaField('Pais', 'STRING'),\n",
    "                        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_Japan, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_Japan está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt\n",
    "google-cloud-bigquery <br>\n",
    "pandas <br>\n",
    "reverse_geocoder <br>\n",
    "requests <br>\n",
    "pyarrow <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEUU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Fecha  Magnitud  Intensidad    GAP    Latitud   \n",
      "2     2023-03-27 04:53:13.815      4.00         3.8  199.0  43.344600  \\\n",
      "4     2023-03-27 04:39:16.340      4.12         3.8  276.0  18.815500   \n",
      "12    2023-03-21 23:18:19.630      4.54         4.1  204.0  40.541500   \n",
      "14    2023-03-20 23:25:28.920      4.90         3.8   32.0  53.790300   \n",
      "29    2023-03-17 03:33:15.894      4.20         2.5  145.0  52.626100   \n",
      "...                       ...       ...         ...    ...        ...   \n",
      "19860 2007-01-03 14:34:38.540      4.40         3.2   42.9  37.067000   \n",
      "19891 2006-12-24 03:43:38.860      4.11         4.3   33.0  33.706833   \n",
      "19908 2006-12-16 06:14:05.350      4.20         4.1   69.0  36.173000   \n",
      "19950 2006-11-29 21:10:55.430      4.11         5.4   48.0  32.820500   \n",
      "19979 2006-11-23 10:42:57.420      4.30         3.9   40.7  37.157000   \n",
      "\n",
      "         Longitud                           Lugar   \n",
      "2     -126.896100      203 km W of Bandon, Oregon  \\\n",
      "4     -155.160333    52 km ESE of Naalehu, Hawaii   \n",
      "12    -124.372500        10km WSW of Ferndale, CA   \n",
      "14    -169.751200  111 km NNW of Nikolski, Alaska   \n",
      "29    -170.072000   88 km WSW of Nikolski, Alaska   \n",
      "...           ...                             ...   \n",
      "19860 -104.895000    8 km SSW of Weston, Colorado   \n",
      "19891 -116.050833          11km NE of Thermal, CA   \n",
      "19908 -120.291667         7km ENE of Coalinga, CA   \n",
      "19950 -115.972500        10km NNE of Ocotillo, CA   \n",
      "19979  -81.975000     13 km NW of Raven, Virginia   \n",
      "\n",
      "                                                    Id  Hipocentro   \n",
      "2                                         ,us7000jmyr,      16.868  \\\n",
      "4                   ,pt23086001,us7000jmyl,hv73350092,       7.790   \n",
      "12     ,ew1679440700,nc73860415,at00rrw8qj,us7000jlua,      18.100   \n",
      "14                ,us7000jljn,at00rrueeh,ak0233n1at5e,     215.062   \n",
      "29                           ,us7000jkqe,ak0233hqts29,      57.861   \n",
      "...                                                ...         ...   \n",
      "19860                          ,usp000f1mr,us2007xcar,       5.000   \n",
      "19891                          ,ci10223765,usp000f0vk,      12.696   \n",
      "19908               ,nc51176849,usp000f0au,ci10222753,      10.210   \n",
      "19950                          ,ci14263712,usp000ez60,       3.621   \n",
      "19979                          ,usp000eym9,us2006vkak,       0.000   \n",
      "\n",
      "           Region Pais        Act  \n",
      "2          Oregon   US 2023-07-12  \n",
      "4          Hawaii   US 2023-07-12  \n",
      "12     California   US 2023-07-12  \n",
      "14         Alaska   US 2023-07-12  \n",
      "29         Alaska   US 2023-07-12  \n",
      "...           ...  ...        ...  \n",
      "19860    Colorado   US 2023-07-12  \n",
      "19891  California   US 2023-07-12  \n",
      "19908  California   US 2023-07-12  \n",
      "19950  California   US 2023-07-12  \n",
      "19979    Virginia   US 2023-07-12  \n",
      "\n",
      "[1906 rows x 12 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1906 entries, 2 to 19979\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   Fecha       1906 non-null   datetime64[ns]\n",
      " 1   Magnitud    1906 non-null   float64       \n",
      " 2   Intensidad  1906 non-null   float64       \n",
      " 3   GAP         1906 non-null   float64       \n",
      " 4   Latitud     1906 non-null   float64       \n",
      " 5   Longitud    1906 non-null   float64       \n",
      " 6   Lugar       1906 non-null   object        \n",
      " 7   Id          1906 non-null   object        \n",
      " 8   Hipocentro  1906 non-null   float64       \n",
      " 9   Region      1906 non-null   object        \n",
      " 10  Pais        1906 non-null   object        \n",
      " 11  Act         1906 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(6), object(4)\n",
      "memory usage: 193.6+ KB\n",
      "None\n",
      "La tabla EEUU_sismo ya existe.\n"
     ]
    }
   ],
   "source": [
    "# Creamos una tabla con los datos de la api en google cloud\n",
    "\n",
    "# Obtener los datos de la API de USGS\n",
    "\n",
    "\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "parameters = {\n",
    "    'format': 'geojson',\n",
    "    'starttime': '1990-01-01',\n",
    "    'endtime': '2023-03-28',\n",
    "    'minmagnitude': '2.5',\n",
    "    'minlatitude': '12.555',\n",
    "    'maxlatitude': '75.141',\n",
    "    'minlongitude': '182.813',\n",
    "    'maxlongitude': '315',\n",
    "    'limit': 20000\n",
    "}\n",
    "response = requests.get(url, params=parameters)\n",
    "data = response.json()\n",
    "\n",
    "# Crear una lista vacía para almacenar los registros\n",
    "records = []\n",
    "\n",
    "# Recorrer los eventos y extraer los parámetros\n",
    "\n",
    "for event in data['features']:\n",
    "    properties = event['properties']\n",
    "    coordinates = event['geometry']['coordinates']\n",
    "    properties['latitude'] = coordinates[1]\n",
    "    properties['longitude'] = coordinates[0]\n",
    "    properties['hipocentro'] = coordinates[2]\n",
    "    records.append(properties)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df_EEUU = pd.DataFrame(records)\n",
    "\n",
    "# Damos formato de fecha a la columna \"time\"\n",
    "df_EEUU[\"time\"] = pd.to_datetime(df_EEUU[\"time\"], unit= \"ms\")\n",
    "\n",
    "\n",
    "\n",
    "# Quitemos los nulos\n",
    "df_EEUU = df_EEUU[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\" ]]\n",
    "df_EEUU = df_EEUU.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creamos las columnas con \"Pais\" y con \"Region\"\n",
    "\n",
    "# Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "coordinates = list(zip(df_EEUU['latitude'], df_EEUU['longitude']))\n",
    "\n",
    "# Realizar la búsqueda inversa de las coordenadas\n",
    "results = rg.search(coordinates)\n",
    "\n",
    "# Obtener la lista de prefecturas a partir de los resultados\n",
    "prefectures = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "# Agregar la columna \"region\" al DataFrame\n",
    "df_EEUU['Region'] = prefectures\n",
    "df_EEUU['Pais'] = countries\n",
    "\n",
    "# Listado de valores a eliminar en la columna 'Col1'\n",
    "paises_permitidos = ['US']\n",
    "\n",
    "# Eliminar las filas donde los valores de 'Pais' no coinciden con el listado\n",
    "df_EEUU = df_EEUU[df_EEUU['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "\n",
    "# Renombramos las columnas\n",
    "nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\",\n",
    "                   'gap': \"GAP\", 'latitude': \"Latitud\", 'longitude': \"Longitud\", \n",
    "                   'place': \"Lugar\", 'ids': \"Id\", \"hipocentro\": 'Hipocentro', 'Region': 'Region' , 'Pais': 'Pais'}\n",
    "df_EEUU.columns = nuevos_nombres.values()\n",
    "\n",
    "# Creamos la columna de la actualización de la data \n",
    "df_EEUU[\"Act\"] = pd.to_datetime(date.today()) \n",
    "\n",
    "\n",
    "# Creamos un archivo json con la data ya normalizada \n",
    "data = df_EEUU\n",
    "\n",
    "\n",
    "# Configurar el proyecto, dataset y tabla\n",
    "project_id = 'henry-sismos'\n",
    "dataset_id = 'Sismos'\n",
    "table_id = 'EEUU_sismo'\n",
    "\n",
    "# Crear la tabla en BigQuery\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "try:\n",
    "    # Intentar obtener la tabla\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"La tabla {table_id} ya existe.\")\n",
    "except NotFound:\n",
    "    # La tabla no existe, crearla\n",
    "    schema = [\n",
    "        bigquery.SchemaField('Fecha', 'TIMESTAMP'),\n",
    "        bigquery.SchemaField('Magnitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Intensidad', 'FLOAT'),\n",
    "        bigquery.SchemaField('GAP', 'FLOAT'),\n",
    "        bigquery.SchemaField('Latitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Longitud', 'FLOAT'),\n",
    "        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "        bigquery.SchemaField('Id', 'STRING'),\n",
    "        bigquery.SchemaField('Region', 'STRING'),\n",
    "        bigquery.SchemaField('Pais', 'STRING'),\n",
    "        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "    ]\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    # Cargar los datos en la tabla\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job = client.load_table_from_dataframe(df_EEUU, table_ref, job_config=job_config)\n",
    "    print(f\"La tabla {table_id} se ha creado correctamente.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue la función que se implemento e google cloud functions carga incremental o actualizacion de informacion\n",
    "\n",
    "import base64\n",
    "from google.cloud import bigquery\n",
    "import requests\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "def actualizar_EEUU_sismo(request, context):\n",
    "    project_id = 'henry-sismos'\n",
    "    dataset_id = 'Sismos'\n",
    "    table_id = 'EEUU_sismo'\n",
    "    \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Crear el cliente de BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Obtener la fecha más reciente en la tabla \"Colombia\"\n",
    "    latest_date_query = f\"SELECT MAX(Fecha) FROM `{project_id}.{dataset_id}.{table_id}`\"\n",
    "    query_job = client.query(latest_date_query)\n",
    "    latest_date_result = query_job.result()\n",
    "\n",
    "    # Obtener el valor de la fecha más reciente\n",
    "    for row in latest_date_result:\n",
    "        latest_date = row[0]\n",
    "\n",
    "    # Verificar si se obtuvo una fecha válida\n",
    "    if latest_date is not None:\n",
    "        latest_date = latest_date.date()\n",
    "    else:\n",
    "        # Si no hay fecha más reciente, establecer una fecha inicial\n",
    "        latest_date = datetime(1990, 1, 1).date()\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Verificar si se necesita actualizar la tabla\n",
    "    if current_date > latest_date:\n",
    "        # Calcular la fecha de inicio y fin para la API\n",
    "        start_date = latest_date + timedelta(days=1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Obtener los datos de la API de USGS\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "        parameters = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': start_date.strftime('%Y-%m-%d'),\n",
    "            'endtime': end_date.strftime('%Y-%m-%d'),\n",
    "            'minmagnitude': '2.5',\n",
    "            'minlatitude': '12.555',\n",
    "            'maxlatitude': '75.141',\n",
    "            'minlongitude': '182.813',\n",
    "            'maxlongitude': '315',\n",
    "            'limit': 20000\n",
    "        }\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Recorrer los eventos y extraer los parámetros\n",
    "        for event in data['features']:\n",
    "            properties = event['properties']\n",
    "            latitude = event['geometry']['coordinates'][1]\n",
    "            longitude = event['geometry']['coordinates'][0]\n",
    "            longitude2 = event['geometry']['coordinates'][2]\n",
    "            properties['latitude'] = latitude\n",
    "            properties['longitude'] = longitude\n",
    "            properties['hipocentro'] = longitude2\n",
    "            records.append(properties)\n",
    "\n",
    "        # Crear el DataFrame\n",
    "        df_EEUU = pd.DataFrame(records)\n",
    "        \n",
    "        # Dar formato de fecha a la columna \"time\"\n",
    "        df_EEUU[\"time\"] = pd.to_datetime(df_EEUU[\"time\"], unit=\"ms\")\n",
    "        # Eliminar filas con valores nulos\n",
    "        df_EEUU = df_EEUU[[\"time\", \"mag\", \"cdi\", \"gap\", \"latitude\", \"longitude\", \"place\", \"ids\", \"hipocentro\"]]\n",
    "        \n",
    "        df_EEUU = df_EEUU.dropna()\n",
    "        \n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if not df_EEUU.empty:\n",
    "            # Verificar la longitud de las listas de coordenadas\n",
    "            if len(df_EEUU['latitude']) == len(df_EEUU['longitude']):\n",
    "                \n",
    "\n",
    "                # Crear una lista de tuplas de coordenadas (latitud, longitud)\n",
    "                coordinates = list(zip(df_EEUU['latitude'], df_EEUU['longitude']))\n",
    "                \n",
    "                # Realizar la búsqueda inversa de las coordenadas\n",
    "                results = rg.search(coordinates)\n",
    "\n",
    "                # Obtener la lista de regiones y países a partir de los resultados\n",
    "                regions = [result['admin1'] if result['admin1'] != '' else 'Desconocido' for result in results]\n",
    "                countries = [result['cc'] if result['cc'] != '' else 'Desconocido' for result in results]\n",
    "                \n",
    "                # Agregar las columnas \"Region\" y \"Pais\" al DataFrame\n",
    "                df_EEUU['Region'] = regions\n",
    "                df_EEUU['Pais'] = countries\n",
    "                \n",
    "                # Listado de valores permitidos en la columna 'Pais'\n",
    "                paises_permitidos = ['US']\n",
    "\n",
    "                # Filtrar las filas donde los valores de 'Pais' no coincidan con el listado\n",
    "                df_EEUU = df_EEUU[df_EEUU['Pais'].isin(paises_permitidos)]\n",
    "\n",
    "                # Renombrar las columnas\n",
    "                nuevos_nombres = {'time': \"Fecha\", 'mag': \"Magnitud\", 'cdi': \"Intensidad\", 'gap': \"GAP\",\n",
    "                                    'latitude': \"Latitud\", 'longitude': \"Longitud\", 'place': \"Lugar\", 'ids': \"Id\" , \"hipocentro\": 'Hipocentro',\n",
    "                                    'Region': 'Region', 'Pais': 'Pais'}\n",
    "                df_EEUU.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "                # Creamos la columna de la actualización de la data \n",
    "                df_EEUU[\"Act\"] = pd.to_datetime(date.today()) \n",
    "\n",
    "\n",
    "                # Insertar los datos en la tabla si hay nuevos datos\n",
    "                if not df_EEUU.empty:\n",
    "                    # Definir el esquema para la tabla de BigQuery\n",
    "                    schema = [\n",
    "                        bigquery.SchemaField(\"Fecha\", \"TIMESTAMP\"),\n",
    "                        bigquery.SchemaField(\"Magnitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Intensidad\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"GAP\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Latitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField(\"Longitud\", \"FLOAT\"),\n",
    "                        bigquery.SchemaField('Hipocentro', 'FLOAT'),\n",
    "                        bigquery.SchemaField('Lugar', 'STRING'),\n",
    "                        bigquery.SchemaField('Id', 'STRING'),\n",
    "                        bigquery.SchemaField('Region', 'STRING'),\n",
    "                        bigquery.SchemaField('Pais', 'STRING'),\n",
    "                        bigquery.SchemaField('Act', 'TIMESTAMP')\n",
    "                    ]\n",
    "\n",
    "                    # Actualizar la tabla en BigQuery\n",
    "                    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "                    job = client.load_table_from_dataframe(df_EEUU, table_ref, job_config=job_config)\n",
    "                    print(\"La tabla ha sido actualizada.\")\n",
    "                else:\n",
    "                    print(\"No hay nuevos datos para insertar.\")\n",
    "\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(\"La longitud de las listas de coordenadas no coincide.\")\n",
    "        else:\n",
    "            print(\"El DataFrame df_EEUU está vacío.\")\n",
    "    else:\n",
    "        print(\"La tabla ya está actualizada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
